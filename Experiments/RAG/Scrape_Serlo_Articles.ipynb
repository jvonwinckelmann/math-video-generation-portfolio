{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4e8417-39cb-48ab-a3f5-3b5ce81f2163",
   "metadata": {},
   "source": [
    "## Serlo Article Scraping for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7baa193-f98b-48d7-b2b7-d6caca4b1201",
   "metadata": {},
   "source": [
    "Um dem Modell im Videoplanungsprozess mehr relevante Informationen an die Hand zu geben, macht es Sinn bereits aufgearbeitete mathematische Artikel als ergänzenden Kontext für die Generierung zu übergeben. Eine Datenquelle für solche Artikel ist de.serlo.org, wo Artikel zu einer Vielzahl mathematischer Themen zur Verfügung stehen. Hier wird exemplarisch ein Ausschnitt von maximal 100 Artikeln dieser Seite gescrapet und in ein RAG System zur Erweiterung des Kontexts des Videoplanungsmodells verwendet.\n",
    "\n",
    "Serlo bietet eine GraphQL Datenbank, über die Metadaten zu Artikeln, Quizzes, etc. bezogen werden können. Aus diesen Metadaten lassen sich auch die Artikel URLs extrahieren, die dann gescrapet und in das RAG System eingepflegt werden können. Ein ähnliches Prinzip kann beispielsweise auch für Wikipedia Artikel verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebfbfd-e0c3-4530-93f7-27b34f3ad083",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f35304-0aad-42f5-8d7c-d5884b16b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Markdown parsing\n",
    "import re\n",
    "import markdownify\n",
    "\n",
    "# Utility\n",
    "import json\n",
    "\n",
    "# VectorDB\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c75f4e-e42d-48cf-a5a0-1e5dfbf2c797",
   "metadata": {},
   "source": [
    "### Metadata Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e2f533-d5c1-44cf-a227-4e56d14f7773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Found 10 Mathematik articles in this turn.\n",
      "Reached the total limit of 100 resources.\n",
      "\n",
      "List of all Mathematik article URLs found:\n",
      "https://serlo.org/1495\n",
      "https://serlo.org/1497\n",
      "https://serlo.org/1499\n",
      "https://serlo.org/1501\n",
      "https://serlo.org/1503\n",
      "https://serlo.org/1505\n",
      "https://serlo.org/1507\n",
      "https://serlo.org/1509\n",
      "https://serlo.org/1511\n",
      "https://serlo.org/1513\n",
      "https://serlo.org/1515\n",
      "https://serlo.org/1517\n",
      "https://serlo.org/1523\n",
      "https://serlo.org/1525\n",
      "https://serlo.org/1527\n",
      "https://serlo.org/1529\n",
      "https://serlo.org/1531\n",
      "https://serlo.org/1533\n",
      "https://serlo.org/1535\n",
      "https://serlo.org/1537\n",
      "https://serlo.org/1539\n",
      "https://serlo.org/1541\n",
      "https://serlo.org/1543\n",
      "https://serlo.org/1545\n",
      "https://serlo.org/1547\n",
      "https://serlo.org/1549\n",
      "https://serlo.org/1551\n",
      "https://serlo.org/1553\n",
      "https://serlo.org/1555\n",
      "https://serlo.org/1559\n",
      "https://serlo.org/1561\n",
      "https://serlo.org/1563\n",
      "https://serlo.org/1565\n",
      "https://serlo.org/1569\n",
      "https://serlo.org/1571\n",
      "https://serlo.org/1573\n",
      "https://serlo.org/1575\n",
      "https://serlo.org/1577\n",
      "https://serlo.org/1579\n",
      "https://serlo.org/1581\n",
      "https://serlo.org/1583\n",
      "https://serlo.org/1585\n",
      "https://serlo.org/1587\n",
      "https://serlo.org/1589\n",
      "https://serlo.org/1591\n",
      "https://serlo.org/1593\n",
      "https://serlo.org/1595\n",
      "https://serlo.org/1597\n",
      "https://serlo.org/1599\n",
      "https://serlo.org/1601\n",
      "https://serlo.org/1603\n",
      "https://serlo.org/1605\n",
      "https://serlo.org/1607\n",
      "https://serlo.org/1609\n",
      "https://serlo.org/1611\n",
      "https://serlo.org/1613\n",
      "https://serlo.org/1615\n",
      "https://serlo.org/1617\n",
      "https://serlo.org/1619\n",
      "https://serlo.org/1621\n",
      "https://serlo.org/1623\n",
      "https://serlo.org/1625\n",
      "https://serlo.org/1627\n",
      "https://serlo.org/1629\n",
      "https://serlo.org/1631\n",
      "https://serlo.org/1633\n",
      "https://serlo.org/1635\n",
      "https://serlo.org/1637\n",
      "https://serlo.org/1639\n",
      "https://serlo.org/1643\n",
      "https://serlo.org/1645\n",
      "https://serlo.org/1647\n",
      "https://serlo.org/1649\n",
      "https://serlo.org/1651\n",
      "https://serlo.org/1653\n",
      "https://serlo.org/1655\n",
      "https://serlo.org/1657\n",
      "https://serlo.org/1659\n",
      "https://serlo.org/1661\n",
      "https://serlo.org/1663\n",
      "https://serlo.org/1665\n",
      "https://serlo.org/1667\n",
      "https://serlo.org/1669\n",
      "https://serlo.org/1671\n",
      "https://serlo.org/1673\n",
      "https://serlo.org/1675\n",
      "https://serlo.org/1677\n",
      "https://serlo.org/1679\n",
      "https://serlo.org/1681\n",
      "https://serlo.org/1683\n",
      "https://serlo.org/1685\n",
      "https://serlo.org/1687\n",
      "https://serlo.org/1689\n",
      "https://serlo.org/1691\n",
      "https://serlo.org/1693\n",
      "https://serlo.org/1695\n",
      "https://serlo.org/1697\n",
      "https://serlo.org/1699\n",
      "https://serlo.org/1701\n",
      "https://serlo.org/1703\n"
     ]
    }
   ],
   "source": [
    "# Retrieve metadata from the Serlo GraphQL API\n",
    "url = \"https://api.serlo.org/graphql\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Function to fetch resources from the API with pagination\n",
    "async def fetch_resources(instance, total_limit=100, first=10, modified_after=None):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        all_resources = []\n",
    "        end_cursor = None\n",
    "        has_next_page = True\n",
    "        fetched_count = 0\n",
    "        article_urls = []\n",
    "\n",
    "        while has_next_page and fetched_count < total_limit:\n",
    "            payload = {\n",
    "                \"query\": \"\"\"\n",
    "                    query($first: Int, $after: String, $instance: Instance, $modifiedAfter: String) {\n",
    "                        metadata {\n",
    "                            resources(first: $first, after: $after, instance: $instance, modifiedAfter: $modifiedAfter) {\n",
    "                                nodes\n",
    "                                pageInfo {\n",
    "                                    hasNextPage\n",
    "                                    endCursor\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                \"\"\",\n",
    "                \"variables\": {\n",
    "                    \"first\": first,\n",
    "                    \"after\": end_cursor,\n",
    "                    \"instance\": instance,\n",
    "                    \"modifiedAfter\": modified_after,\n",
    "                }\n",
    "            }\n",
    "\n",
    "            response = await client.post(url, headers=headers, json=payload)\n",
    "            response.raise_for_status()  # Raise an error for HTTP issues\n",
    "            data = response.json()\n",
    "\n",
    "            resources = data['data']['metadata']['resources']\n",
    "            nodes = resources['nodes']\n",
    "            all_resources.extend(nodes)\n",
    "\n",
    "            # Filter articles related to \"Mathematik\"\n",
    "            matematik_articles = filter_articles(nodes)\n",
    "            for article in matematik_articles:\n",
    "                article_urls.append(article['id'])  # Store the resource[\"id\"] as URL\n",
    "\n",
    "            # Print how many Mathematik articles were found in this turn\n",
    "            print(f\"Found {len(matematik_articles)} Mathematik articles in this turn.\")\n",
    "\n",
    "            # Update the count of fetched resources\n",
    "            fetched_count += len(nodes)\n",
    "\n",
    "            # Update pagination variables\n",
    "            has_next_page = resources['pageInfo']['hasNextPage']\n",
    "            end_cursor = resources['pageInfo']['endCursor']\n",
    "\n",
    "            # Stop if we have fetched the desired number of resources\n",
    "            if fetched_count >= total_limit:\n",
    "                print(f\"Reached the total limit of {total_limit} resources.\")\n",
    "                break\n",
    "\n",
    "        return article_urls\n",
    "\n",
    "# Function to filter resources by type (Article)\n",
    "def filter_articles(resources):\n",
    "    return [resource for resource in resources if \"Article\" in resource.get('type', [])]\n",
    "\n",
    "# Main function to run the script\n",
    "async def main():\n",
    "    instance = \"de\"\n",
    "    about_id_to_filter = \"http://w3id.org/kim/schulfaecher/s1017\"  # Mathematik about_id\n",
    "\n",
    "    # Fetch up to 100 resources\n",
    "    article_urls = await fetch_resources(instance, total_limit=100)\n",
    "\n",
    "    article_urls_df = pd.DataFrame({\"article_urls\": article_urls})\n",
    "\n",
    "    article_urls_df.to_csv(\"article_urls.csv\", index=False)\n",
    "    \n",
    "    # Output the list of all Mathematik article URLs\n",
    "    print(\"\\nList of all Mathematik article URLs found:\")\n",
    "    for url in article_urls:\n",
    "        print(url)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee374d-60d4-479e-8f71-1575e1062bcb",
   "metadata": {},
   "source": [
    "Im nächsten Schritt können die Artikel gescrapet und in Markdown überführt werden, damit das LLM mit den generierten Chunks besser arbeiten kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6edaa0-3b95-478e-b9d6-ccea0bed5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_markdown_by_headlines(markdown_content):\n",
    "    \"\"\"Chunk the Markdown content based on headers\"\"\"\n",
    "    \n",
    "    # Regular expression to match Markdown headers (e.g., #, ##, ###)\n",
    "    header_pattern = re.compile(r'^(#{1,6})\\s+(.*)', re.MULTILINE)\n",
    "    \n",
    "    # Split the content based on the headers\n",
    "    chunks = []\n",
    "    last_idx = 0\n",
    "    for match in header_pattern.finditer(markdown_content):\n",
    "        # Get the header level (e.g., #, ##, ###)\n",
    "        header_level = match.group(1)\n",
    "        header_text = match.group(2)\n",
    "        \n",
    "        # Add the content under each header as a new chunk\n",
    "        if last_idx != 0:\n",
    "            chunk = markdown_content[last_idx:match.start()].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        # Update the index to the start of the current header\n",
    "        last_idx = match.end()\n",
    "\n",
    "        # Add the header itself to the chunk\n",
    "        chunks.append(f\"{header_level} {header_text}\")\n",
    "    \n",
    "    # Add the last chunk if any content remains after the last header\n",
    "    if last_idx != len(markdown_content):\n",
    "        chunks.append(markdown_content[last_idx:].strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def clean_markdown(markdown_content):\n",
    "    \"\"\"Clean markdown content by removing various markdown elements and normalizing text.\"\"\"\n",
    "    def replace_links(match):\n",
    "        return match.group(1) or match.group(2)\n",
    "    \n",
    "    content = markdown_content\n",
    "    content = re.sub(r'\\[([^\\]]+)\\]\\(([^)]+)\\)', replace_links, content)\n",
    "    content = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', content)\n",
    "    content = re.sub(r'[*_]{1,3}([^*_]+)[*_]{1,3}', r'\\1', content)\n",
    "    content = re.sub(r'```[^`]*```', '', content)\n",
    "    content = re.sub(r'`[^`]+`', '', content)\n",
    "    content = re.sub(r'<[^>]+>', '', content)\n",
    "    content = re.sub(r'^\\s*>\\s*', '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Normalize whitespace while preserving line breaks\n",
    "    content = re.sub(r' +', ' ', content)\n",
    "    content = re.sub(r'^\\s+|\\s+$', '', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'\\n\\s*\\n', '\\n\\n', content)\n",
    "    return content.strip()\n",
    "\n",
    "def remove_headlines(chunks):\n",
    "    \"\"\"Remove headlines from chunks.\"\"\"\n",
    "    return [chunk for chunk in chunks if not chunk.startswith(\"#\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9878c07-079a-41ec-8b68-8422a839e52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://serlo.org/1495', 'https://serlo.org/1497', 'https://serlo.org/1499']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load article urls\n",
    "article_urls_df = pd.read_csv(\"article_urls.csv\")\n",
    "\n",
    "article_urls = article_urls_df[\"article_urls\"].to_list()\n",
    "article_urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a1b8dc-8685-4316-a5e5-060e217572b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the content\n",
    "async def scrape_content(url: str):\n",
    "    async with httpx.AsyncClient(follow_redirects=True) as client:\n",
    "        # Fetch the page content with automatic redirection handling\n",
    "        response = await client.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad responses\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the <main> tag with id=\"content\"\n",
    "        content = soup.find('main', id='content')\n",
    "        \n",
    "        if content:\n",
    "            return content\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Function to convert HTML to markdown\n",
    "def html_to_markdown(html_content):\n",
    "    return markdownify.markdownify(str(html_content), heading_style=\"ATX\")\n",
    "\n",
    "# Check if the environment is already running an event loop\n",
    "def run_scrape(url: str):\n",
    "    try:\n",
    "        return asyncio.ensure_future(scrape_content(url))\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(scrape_content(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64abb90-2759-44a2-859f-aead76c6553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape and chunk all articles\n",
    "serlo_data = {\n",
    "    \"article_urls\": [],\n",
    "    \"markdown_contents\": [],\n",
    "    \"content_chunks\": []\n",
    "}\n",
    "\n",
    "for article_url in article_urls:\n",
    "    # Scrape content\n",
    "    content = await scrape_content(article_url)\n",
    "    \n",
    "    if content:\n",
    "        # Convert content to markdown\n",
    "        markdown_content = html_to_markdown(content)\n",
    "\n",
    "        # Chunk content\n",
    "        chunks = chunk_markdown_by_headlines(markdown_content)\n",
    "\n",
    "        # Clean chunks from markdown links, etc.\n",
    "        cleaned_chunks = [clean_markdown(chunk) for chunk in chunks]\n",
    "\n",
    "        # Filter only for non-headline chunks\n",
    "        content_chunks = remove_headlines(cleaned_chunks)\n",
    "        \n",
    "        serlo_data[\"article_urls\"].append(article_url)\n",
    "        serlo_data[\"markdown_contents\"].append(markdown_content)\n",
    "        serlo_data[\"content_chunks\"].append(content_chunks)\n",
    "    else:\n",
    "        print(\"Content not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5a0342-540a-4cc2-9a12-80d90fb1d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store articles in JSON file\n",
    "with open(\"serlo_data.json\", \"w\") as f:\n",
    "    json.dump(serlo_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f39076b-786e-4c5c-af7b-122cca3c85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence transformer model\n",
    "dimensions = 1024\n",
    "model = SentenceTransformer(\"mixedbread-ai/deepset-mxbai-embed-de-large-v1\", truncate_dim=dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c24a05-e948-4382-a477-0bc15625a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChromaDB persistent client\n",
    "client = chromadb.PersistentClient(path=\"vectordb\")\n",
    "\n",
    "# Create a collection (if not already created)\n",
    "collection = client.create_collection(\"serlo_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddfef102-8ad4-4356-b242-dba4faa8544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add content chunks to vector database\n",
    "for i, article_url in enumerate(serlo_data[\"article_urls\"]):\n",
    "    content_chunks = serlo_data[\"content_chunks\"][i]\n",
    "    \n",
    "    embeddings = model.encode(content_chunks)\n",
    "\n",
    "    collection.add(\n",
    "        documents=content_chunks,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"source\": article_url} for _ in content_chunks],\n",
    "        ids=[str(article_url.split(\"/\")[-1]) + \"_\" + str(i) for i, _ in enumerate(content_chunks)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07cb8410-bcb8-48ad-96c5-66824271af46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Zeichne den Punkt P(4|6) in ein Koordinatensystem ein.\n",
      "Top 3 most similar documents:\n",
      "\n",
      "Kontext:\n",
      "Die Lageinformation eines Punktes im zweidimensionalen Koordinatensystem wird in runden Klammern geschrieben und durch einen senkrechten Strich getrennt:Nullpunkt: (0∣0)(0|0)(0∣0)\n",
      "Punkt P=(3∣4)P = (3|4)P=(3∣4)\n",
      "* Punkt Q=(−2∣1)Q=(-2|1)Q=(−2∣1)Zusätzlich kann man von einem Punkt den Quadranten angeben.!Bild\n",
      "\n",
      "Kontext:\n",
      "LadenWeitere Aufgaben zum Thema findest du im folgenden Aufgabenordner:\n",
      "Aufgaben zum Koordinatensystem\n",
      "\n",
      "Kontext:\n",
      "Die Lageinformation eines Punktes im zweidimensionalen Koordinatensystem wird in runden Klammern geschrieben und durch einen senkrechten Strich getrennt.Die Stelle vor dem senkrechten Strich ist die xxx-Koordinate, die man an der xxx-Achse abzählt.Die Stelle nach dem senkrechten Strich ist die yyy-Koordinate und wird an der yyy-Achse abgezählt.Beispiel im Bild:Nullpunkt: O(0∣0)\\color{#660099}{O(0|0)}O(0∣0)\n",
      "Punkt P(2∣3)\\color{#cc0000}{P(2|3)}P(2∣3)\n",
      "Punkt Q(−2∣1)\\color{#006400}{Q(-2|1)}Q(−2∣1)(Die Hilfslinien im Bild sind nur eine Unterstützung. Du musst diese nicht einzeichnen.)Man nennt die xxx-Koordinate auch Abszisse, die yyy-Koordinate auch Ordinate.!BildIn der folgenden Animation kannst du den Punkt P\\color{#009999}{P}P in jede beliebige Richtung verschieben. Du siehst die Koordinaten des Punkts jeweils daneben angezeigt.!Vorschaubild GeoGebra Applet laden von GeoGebraMit einem Klick auf Bild oder Button oben stimmst du zu, dass externe Inhalte von GeoGebra** geladen werden. Dabei können persönliche Daten zu diesem Service übertragen werden – entsprechend unserer Datenschutzerklärung.[\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"Zeichne den Punkt P(4|6) in ein Koordinatensystem ein.\"\n",
    "\n",
    "# Encode the query\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Retrieve similar documents from the collection\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3,\n",
    "    include=['documents', 'metadatas']\n",
    ")\n",
    "\n",
    "# Print out the results\n",
    "print(\"Query:\", query)\n",
    "\n",
    "print(\"Top 3 most similar documents:\")\n",
    "print()\n",
    "for documents in results[\"documents\"]:\n",
    "    context = \"Kontext:\\n\"\n",
    "    context += \"\\n\\nKontext:\\n\".join(documents)\n",
    "    print(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
