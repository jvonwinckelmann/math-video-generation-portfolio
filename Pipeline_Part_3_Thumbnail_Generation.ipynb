{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e20a96",
   "metadata": {},
   "source": [
    "## Thumbnail Generation Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd62626",
   "metadata": {},
   "source": [
    "Um den Prozess abzuschließen soll für das generierte Video noch ein passendes Thumbnail generiert werden, dass eventuell etwas interessanter aussieht als einfach eine Stelle des Videos. Dafür generiert ein Sprachmodell Ideen, die dann in Function Calls und letztlich Prompts und generierte Bilder überführt werden, von denen dann die geeigneten Bilder durch das Vision Language Model ausgewählt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7db56",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920cdd86-f4e8-4af3-ae18-ce60b59fd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusers Models\n",
    "import torch\n",
    "from diffusers import FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# LlamaIndex Workflow\n",
    "from llama_index.core.workflow import Event, StartEvent, StopEvent, Workflow, step, Context\n",
    "from llama_index.utils.workflow import draw_all_possible_flows, draw_most_recent_execution # to visualize the Workflow\n",
    "\n",
    "# Ollama LLM\n",
    "import ollama\n",
    "\n",
    "# Typing and Structured Output Models\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Utility Libraries\n",
    "import json\n",
    "import os\n",
    "from IPython.display import Image, display # for displaying generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4262e4b-5529-4911-b428-36b201cc5c20",
   "metadata": {},
   "source": [
    "### Load Video Plan\n",
    "\n",
    "Lade Resulte aus dem ersten Part der Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd43f2a-a0e3-47b2-b0e4-d3baaca03f6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/part_1_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load part 1 results (task description and video plan)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m part_1_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/part_1_results.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     part_1_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      7\u001b[0m task_description \u001b[38;5;241m=\u001b[39m part_1_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_description\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/torch_directory/.env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/part_1_results.json'"
     ]
    }
   ],
   "source": [
    "# Load part 1 results (task description and video plan)\n",
    "part_1_results = {}\n",
    "\n",
    "try:\n",
    "    with open('results/part_1_results.json', 'r') as f:\n",
    "        part_1_results = json.load(f)\n",
    "    \n",
    "    task_description = part_1_results.get('task_description', None)\n",
    "    video_plan = part_1_results.get('video_plan', None)\n",
    "except:\n",
    "    task_description = part_1_results.get('task_description', None)\n",
    "    video_plan = part_1_results.get('video_plan', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de507ccc",
   "metadata": {},
   "source": [
    "### Thumbnail Generation Workflow Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c35575-d9ba-4300-9ecd-d9ca709fd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thumbnail Generation Settings\n",
    "thumbnail_generation_settings = {\n",
    "    \"project_name\": \"project-1\",\n",
    "    \"diffusion_model\": \"shuttle\",\n",
    "    \"code_llm\": \"qwen2.5-coder:latest\",\n",
    "    \"vlm\": \"llama3.2-vision:latest\",\n",
    "    \"debug_mode\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e572cc",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332665c0-b8a4-4588-9141-06be92839904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize a quantized Shuttle Diffusion Pipeline\n",
    "def initialize_shuttle_diffusion_pipeline():\n",
    "    ckpt_path = \"https://huggingface.co/shuttleai/shuttle-3.1-aesthetic/blob/main/gguf/shuttle-3.1-aesthetic-Q4_K_S.gguf\"\n",
    "    \n",
    "    transformer = FluxTransformer2DModel.from_single_file(\n",
    "        ckpt_path,\n",
    "        quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        \"shuttleai/shuttle-3.1-aesthetic\",\n",
    "        transformer=transformer,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "# Function to initialize a Stable Diffusion XL Pipeline\n",
    "def initialize_sdxl_pipeline():\n",
    "\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    return pipe\n",
    "    \n",
    "# Initialize the pipeline\n",
    "if thumbnail_generation_settings[\"diffusion_model\"] == \"shuttle\":\n",
    "    pipe = initialize_shuttle_diffusion_pipeline()\n",
    "else:\n",
    "    pipe = initialize_sdxl_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383e1b4-1a39-4e90-8389-fd4c647f21bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image tool\n",
    "def generate_image(prompt: str, file_name: str):\n",
    "    \"\"\"\n",
    "    Generates an image based on a given prompt and saves the result under the given file name\n",
    "    \n",
    "    Args:\n",
    "    prompt: AI generated prompt to generate an image using a diffusion model\n",
    "    file_name: Name used to save the generated without file extension\n",
    "    \n",
    "    Returns:\n",
    "    str: The whole path where the image file was saved\n",
    "    \"\"\"\n",
    "    current_folder = os.getcwd()\n",
    "    output_dir = current_folder + \"/thumbnails/\" + thumbnail_generation_settings[\"project_name\"]\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    if thumbnail_generation_settings[\"diffusion_model\"] == \"shuttle\":\n",
    "        num_inference_steps = 4\n",
    "    else:\n",
    "        num_inference_steps = 50\n",
    "        \n",
    "    height = 288\n",
    "    width = 512\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=10.0,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=torch.manual_seed(42)\n",
    "    ).images[0]\n",
    "    \n",
    "    file_path = output_dir + \"/\" + file_name + \".png\"\n",
    "    image.save(file_path)\n",
    "    return file_path\n",
    "\n",
    "generate_image(\"A House\", \"house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2b70b-ac8f-49dc-b346-b3b6f47a4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama LLM Utility Function with optional image input, function calling and structured output generation\n",
    "def call_ollama_model(\n",
    "    model=\"qwen2.5-coder:latest\",\n",
    "    system_prompt=None,\n",
    "    user_prompt=None,\n",
    "    temperature=0.1,\n",
    "    context_size=16384,\n",
    "    images=None,\n",
    "    tools=None,\n",
    "    json_schema=None\n",
    "):\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "                \"images\": images\n",
    "            }\n",
    "        ],\n",
    "        options={\n",
    "            \"temperature\": temperature,\n",
    "            \"num_ctx\": context_size\n",
    "        },\n",
    "        format=json_schema,\n",
    "        tools=tools\n",
    "    )\n",
    "    \n",
    "    return response.message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32460c",
   "metadata": {},
   "source": [
    "### Model and Event Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a05ff-6502-49a2-8cca-a0637fadd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for structured output generation of thumbnail ideas\n",
    "class ThumbnailIdeas(BaseModel):\n",
    "    thumbnail_ideas: List[str]\n",
    "\n",
    "# Model for structured output generation of image suitability as thumbnail\n",
    "class ImageSuitability(BaseModel):\n",
    "    is_suitable: bool\n",
    "    explanation: str\n",
    "\n",
    "# Event that thumbnail ideas are generated\n",
    "class ThumbnailIdeasGeneratedEvent(Event):\n",
    "    thumbnail_ideas: ThumbnailIdeas\n",
    "\n",
    "# Event that images were generated\n",
    "class ImagesGeneratedEvent(Event):\n",
    "    image_paths: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompts for thumbnail generation workflow\n",
    "import thumbnail_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example system prompt\n",
    "thumbnail_system_prompts = thumbnail_prompts.SystemPrompts()\n",
    "print(thumbnail_system_prompts.THUMBNAIL_IDEA_GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d817eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user prompt\n",
    "thumbnail_user_prompts = thumbnail_prompts.UserPrompts()\n",
    "print(thumbnail_user_prompts.THUMBNAIL_IDEA_GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa81c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user prompt params\n",
    "user_prompts = thumbnail_prompts.UserPrompts()\n",
    "print(user_prompts.get_params(\"THUMBNAIL_IDEA_GENERATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c87e5c-10dc-4584-9c95-05c0cf69f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThumbnailGenerationFlow(Workflow):\n",
    "    @step\n",
    "    async def generate_thumbnail_ideas(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> ThumbnailIdeasGeneratedEvent:\n",
    "        # Get input and settings from StartEvent\n",
    "        topic = ev.topic\n",
    "        video_plan = ev.video_plan\n",
    "        num_thumbnails = ev.num_thumbnails\n",
    "\n",
    "        settings = ev.settings\n",
    "        code_llm = settings.get(\"code_llm\", \"qwen2.5-coder:latest\")\n",
    "        vlm = settings.get(\"vlm\", \"llama3.2-vision:latest\")\n",
    "        debug_mode = settings.get(\"debug_mode\", False)\n",
    "\n",
    "        # Set initial global context\n",
    "        await ctx.set(\"code_llm\", code_llm)\n",
    "        await ctx.set(\"vlm\", vlm)\n",
    "        await ctx.set(\"debug_mode\", debug_mode)\n",
    "\n",
    "        system_prompt = thumbnail_system_prompts.THUMBNAIL_IDEA_GENERATION\n",
    "        user_prompt = thumbnail_user_prompts.THUMBNAIL_IDEA_GENERATION.format(\n",
    "            num_thumbnails=num_thumbnails, topic=topic, video_plan=video_plan\n",
    "        )\n",
    "\n",
    "        response_msg = call_ollama_model(\n",
    "            model=vlm,\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            images=None,\n",
    "            temperature=0.1,\n",
    "            context_size=16384,\n",
    "            json_schema=ThumbnailIdeas.model_json_schema(),\n",
    "        )\n",
    "\n",
    "        # Try to parse the generated JSON response\n",
    "        try:\n",
    "            thumbnail_ideas = json.loads(response_msg.content)\n",
    "        except:\n",
    "            thumbnail_ideas = ThumbnailIdeas(thumbnail_ideas=[])\n",
    "\n",
    "        if debug_mode:\n",
    "            print(thumbnail_ideas)\n",
    "            \n",
    "        return ThumbnailIdeasGeneratedEvent(thumbnail_ideas=thumbnail_ideas)\n",
    "\n",
    "    @step\n",
    "    async def generate_images(\n",
    "        self, ctx: Context, ev: ThumbnailIdeasGeneratedEvent\n",
    "    ) -> ImagesGeneratedEvent:\n",
    "        # Get input and settings from ThumbnailIdeasGeneratedEvent\n",
    "        thumbnail_ideas = ev.thumbnail_ideas\n",
    "\n",
    "        # Get relevant data from global context\n",
    "        code_llm = await ctx.get(\"code_llm\")\n",
    "        debug_mode = await ctx.get(\"debug_mode\")\n",
    "\n",
    "        # Get the list of thumbnail ideas and turn it into a string as context\n",
    "        thumbnail_ideas_list = thumbnail_ideas.thumbnail_ideas\n",
    "        thumbnail_ideas_str = \"\\n\\n\".join(thumbnail_ideas_list)\n",
    "\n",
    "        system_prompt = thumbnail_system_prompts.IMAGE_GENERATION_FUNCTION_CALLING\n",
    "        user_prompt = thumbnail_user_prompts.IMAGE_GENERATION_FUNCTION_CALLING.format(\n",
    "            thumbnail_ideas_str=thumbnail_ideas_str\n",
    "        )\n",
    "\n",
    "        response_msg = call_ollama_model(\n",
    "            model=code_llm,\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            images=None,\n",
    "            temperature=0.1,\n",
    "            context_size=16384,\n",
    "            tools=[generate_image],\n",
    "        )\n",
    "\n",
    "        available_functions = {\n",
    "            \"generate_image\": generate_image,\n",
    "        }\n",
    "\n",
    "        image_paths = []\n",
    "\n",
    "        # Gerate images via function call\n",
    "        for tool in response_msg.tool_calls or []:\n",
    "            function_to_call = available_functions.get(tool.function.name)\n",
    "            if function_to_call:\n",
    "                arguments = tool.function.arguments\n",
    "                file_path = function_to_call(**arguments)\n",
    "                image_paths.append(file_path)\n",
    "\n",
    "                if debug_mode:\n",
    "                    print(\"Prompt: \", arguments.get(\"prompt\", \"\"))\n",
    "                    print(\"Function output:\", file_path)\n",
    "                    display(Image(filename=file_path))\n",
    "            else:\n",
    "                print(\"Function not found:\", tool.function.name)\n",
    "\n",
    "        return ImagesGeneratedEvent(image_paths=image_paths)\n",
    "\n",
    "    @step\n",
    "    async def evaluate_images(\n",
    "        self, ctx: Context, ev: ImagesGeneratedEvent\n",
    "    ) -> StopEvent:\n",
    "        # Get input and settings from StartEvent\n",
    "        image_paths = ev.image_paths\n",
    "\n",
    "        # Get relevant data from global context\n",
    "        debug_mode = await ctx.get(\"debug_mode\")\n",
    "        vlm = await ctx.get(\"vlm\")\n",
    "\n",
    "        system_prompt = thumbnail_system_prompts.THUMBNAIL_EVALUATION\n",
    "        user_prompt = thumbnail_user_prompts.THUMBNAIL_EVALUATION\n",
    "\n",
    "        suitable_images = []\n",
    "\n",
    "        # Check image suitability as thumbnail (one by one as Llama3.2 Vision only takes 1 image at a time in Ollama)\n",
    "        for image_path in image_paths:\n",
    "            response_msg = call_ollama_model(\n",
    "                model=vlm,\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                images=[image_path],\n",
    "                temperature=0.1,\n",
    "                context_size=16384,\n",
    "                json_schema=ImageSuitability.model_json_schema(),\n",
    "            )\n",
    "\n",
    "            image_suitability = json.loads(response_msg.content)\n",
    "            is_suitable = image_suitability.get(\"is_suitable\", False)\n",
    "\n",
    "            if debug_mode:\n",
    "                print(\"Thumbnail is suitable: \", str(is_suitable))\n",
    "                print(image_suitability[\"explanation\"])\n",
    "\n",
    "            if is_suitable:\n",
    "                suitable_images.append(image_path)\n",
    "\n",
    "        return StopEvent(result=suitable_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713185f5-009e-4954-8165-30410c43943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PLAN = \"\"\"1. **Create Grid**: Draw a coordinate system on paper or screen. Mark the x and y axes for orientation. The grid should be large enough to display all given points.\n",
    "\n",
    "2. **Mark Points**:\n",
    "   - **Point A**: Locate (1, 1) on the coordinate system. Move 1 unit along the x-axis from the origin to the right and then 1 unit up along the y-axis. Mark this point as A.\n",
    "   - **Point B**: Find (5, 1) on the coordinate system. Move 5 units along the x-axis from the origin to the right and stay at the same height (y=1). Mark this point as B.\n",
    "   - **Point C**: Find (3, 4) on the coordinate system. Move 3 units along the x-axis from the origin to the right and then 4 units up along the y-axis. Mark this point as C.\n",
    "\n",
    "3. **Label Points**: Label each marked point with its corresponding letter to clearly identify them: Label the point at (1, 1) as A, at (5, 1) as B, and at (3, 4) as C.\n",
    "\n",
    "4. **Draw Lines**:\n",
    "   - Connect Point A and Point B with a straight line.\n",
    "   - Connect Point B and Point C with a straight line.\n",
    "   - Connect Point C and Point A with a straight line.\n",
    "\n",
    "5. **Verification**:\n",
    "   - Verify that each triangle segment is drawn correctly: AB horizontal (y-coordinates equal), AC and BC diagonal in their respective directions based on their x and y coordinates.\n",
    "\n",
    "6. **Summary**: You should now have a closed triangle, labeled with A(1, 1), B(5, 1), and C(3, 4), within your coordinate system.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9c743-0ec1-48b5-85a4-b530eead91ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thumbnail_generation_flow = ThumbnailGenerationFlow(timeout=None, verbose=True)\n",
    "\n",
    "thumbnail_generation_ctx = Context(thumbnail_generation_flow)\n",
    "\n",
    "draw_all_possible_flows(ThumbnailGenerationFlow, filename=\"Pipeline-Visualizations/thumbnail_generation_flow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe6512-d341-4966-9ef1-f383157facc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await thumbnail_generation_flow.run(task_description=task_description, video_plan=video_plan, num_thumbnails=3, settings=thumbnail_generation_settings)\n",
    "\n",
    "draw_most_recent_execution(thumbnail_generation_flow, filename=\"Pipeline-Visualizations/thumbnail_generation_last_execution.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc83a9c-f088-41e7-b1c5-88da4d877899",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
