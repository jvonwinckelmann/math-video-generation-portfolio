{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning mit Unloth AI\n",
    "\n",
    "Quelle: https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing#scrollTo=rkp0uMrNpYaW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unsloth import FastLanguageModel, to_sharegpt, standardize_sharegpt\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "import subprocess\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **1. Setup und Vorbereitung**\n",
    "# Installiere die notwendigen Bibliotheken und richte die Umgebung ein.\n",
    "# Hier wird Unsloth installiert, eine Bibliothek zur Optimierung von Sprachmodellen.\n",
    "!pip install unsloth\n",
    "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeremy Modell anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **2. Modell Laden**\n",
    "# Lade ein vortrainiertes Sprachmodell und seinen zugehörigen Tokenizer.\n",
    "# `max_seq_length`: Maximale Sequenzlänge, die das Modell verarbeiten kann.\n",
    "# `load_in_4bit`: Aktiviert die Verwendung von 4-Bit-Quantisierung, um Speicherplatz zu sparen.\n",
    "# `dtype`: Datentyp für die Berechnung, hier wird der Standard des Modells verwendet.\n",
    "\n",
    "max_seq_length = 2048\n",
    "load_in_4bit = True\n",
    "dtype = None  # Automatische Erkennung basierend auf Hardware und Modell\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",  # Ersetze durch das gewünschte vortrainierte Modell\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeremy Datensatz einbinden vorgabe siehe 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **3. Vorbereitung der eigenen Daten**\n",
    "# Lade den Datensatz, der für das Finetuning verwendet werden soll.\n",
    "# Hier wird ein CSV-Dateiformat erwartet. Passe den Dateinamen und Split entsprechend an.\n",
    "data_file = \"deine_datei.csv\"  # Pfad zur Datei\n",
    "dataset = load_dataset(\n",
    "    \"csv\",              # Dateityp (z. B. CSV oder Excel)\n",
    "    data_files=data_file,  # Dateipfad\n",
    "    split=\"train\"       # Trainingssplit laden\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datensatz struktur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **4. Datentransformation**\n",
    "# Konvertiere den Datensatz in ein ShareGPT-ähnliches Format, das für Chatmodelle geeignet ist.\n",
    "# Passe die Namen der Eingabe- und Zielspalten sowie das Format des Prompts an deine Datenstruktur an.\n",
    "merged_prompt = \"\"\"[[Die Eingabe ist {input_col}.]]\n",
    "                   [[Erwartete Ausgabe ist {output_col}.]]\"\"\"  # Definiert die Struktur des Prompts\n",
    "output_column_name = \"output_col\"  # Spaltenname für die erwartete Ausgabe\n",
    "\n",
    "# Wende die Transformation auf den Datensatz an\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt=merged_prompt,  # Das definierte Prompt-Format\n",
    "    output_column_name=output_column_name,  # Zielspalte\n",
    "    conversation_extension=5,  # Optional: Anzahl zusätzlicher Konversationsturns\n",
    ")\n",
    "dataset = standardize_sharegpt(dataset)  # Standardisiere das Format des Datensatzes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **5. Training Konfigurieren und Starten**\n",
    "# Konfiguriere die Trainingsparameter wie Batch-Größe, Lernrate und maximale Schritte.\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,  # Anzahl der Trainingsinstanzen pro Gerät und Schritt\n",
    "    gradient_accumulation_steps=4,  # Schritte, bevor die Gradienten aktualisiert werden\n",
    "    warmup_steps=5,  # Anzahl der Warmup-Schritte für die Lernrate\n",
    "    max_steps=60,  # Maximale Anzahl an Trainingsschritten\n",
    "    learning_rate=2e-4,  # Anfangslernrate\n",
    "    fp16=True,  # Mixed Precision Training für schnellere Berechnungen\n",
    "    logging_steps=1,  # Wie oft Logs geschrieben werden (in Schritten)\n",
    "    output_dir=\"outputs\",  # Speicherort für Modell, Checkpoints und Logs\n",
    "    seed=42,  # Zufallssaat für reproduzierbare Ergebnisse\n",
    "    optim=\"adamw_8bit\",  # Optimierer, der für 8-Bit-Gewichte geeignet ist\n",
    ")\n",
    "\n",
    "# Erstelle den Trainer, um das Modell zu finetunen\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Vortrainiertes Modell\n",
    "    tokenizer=tokenizer,  # Passender Tokenizer\n",
    "    train_dataset=dataset,  # Der vorbereitete Datensatz\n",
    "    dataset_text_field=\"text\",  # Feldname für den Text im Datensatz\n",
    "    max_seq_length=max_seq_length,  # Maximale Sequenzlänge\n",
    "    dataset_num_proc=2,  # Anzahl paralleler Prozesse für Datenvorverarbeitung\n",
    "    args=training_args,  # Trainingsparameter\n",
    ")\n",
    "\n",
    "# Starte das Training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **6. Modell Speichern**\n",
    "# Speichere das finetunete Modell und den zugehörigen Tokenizer in einem Verzeichnis.\n",
    "save_dir = \"finetuned_model\"\n",
    "model.save_pretrained(save_dir)  # Speichere das Modell\n",
    "tokenizer.save_pretrained(save_dir)  # Speichere den Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **7. Export für Ollama**\n",
    "# Exportiere das Modell im GGUF-Format, um es in Ollama verwenden zu können.\n",
    "# Wähle die gewünschte Quantisierungsmethode (z. B. `q8_0` für 8-Bit-Quantisierung).\n",
    "quantization_method = \"q8_0\"  # Optionen: \"q4_k_m\", \"q5_k_m\", \"f16\"\n",
    "model.save_pretrained_gguf(save_dir, tokenizer, quantization_method=quantization_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **8. Ollama Server Starten**\n",
    "# Starte den Ollama-Server, um das Modell lokal verfügbar zu machen.\n",
    "subprocess.Popen([\"ollama\", \"serve\"])  # Startet den Server-Prozess\n",
    "time.sleep(3)  # Warte, bis der Server vollständig gestartet ist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **9. Modelfile Generieren und Modell in Ollama Importieren**\n",
    "# Generiere eine Modellkonfigurationsdatei für Ollama und importiere das Modell.\n",
    "ollama_modelfile = tokenizer._ollama_modelfile  # Hole die Modelfile-Information\n",
    "with open(os.path.join(save_dir, \"Modelfile\"), \"w\") as f:\n",
    "    f.write(ollama_modelfile)  # Schreibe die Datei\n",
    "\n",
    "# Importiere das Modell in Ollama\n",
    "subprocess.run([\"ollama\", \"create\", \"unsloth_model\", \"-f\", os.path.join(save_dir, \"Modelfile\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **10. Inferenz in Ollama**\n",
    "# Sende eine Anfrage an den Ollama-Server, um das Modell zu testen.\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Die Eingabe ist Beispieltext. Erwartete Ausgabe ist Beispielantwort.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Sende die Anfrage an den Ollama-Server\n",
    "response = subprocess.run([\n",
    "    \"curl\",\n",
    "    \"http://localhost:11434/api/chat\",\n",
    "    \"-d\",\n",
    "    f\"\"\"\n",
    "    {{\n",
    "        \"model\": \"unsloth_model\",\n",
    "        \"messages\": {messages}\n",
    "    }}\n",
    "    \"\"\"\n",
    "], capture_output=True)\n",
    "\n",
    "# Ausgabe der Antwort des Servers\n",
    "print(response.stdout.decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
